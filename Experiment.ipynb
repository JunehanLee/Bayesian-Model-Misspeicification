{"cells":[{"cell_type":"markdown","metadata":{"id":"4iVAjs1xo4Pm"},"source":["#  Experimental Design: Structural Misspecification in Choice Models\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wLRCfIwno_mD"},"source":["## Experiment Overview\n","\n","This notebook evaluates the robustness and performance of discrete choice models under different types of model misspecification. We explore two key scenarios:\n","\n","---\n","\n","## **Scenario 1: Structural Misspecification**\n","\n","We begin with a simple baseline logistic regression model and assess the impact of adding one structural component at a time. This helps us understand how each feature affects model performance, calibration, and posterior uncertainty.\n","\n","### Components added individually:\n","\n","- **Group-level effect**  \n","  Models unobserved heterogeneity by allowing group-specific intercepts or coefficients.\n","\n","- **Interaction term**  \n","  Captures multiplicative relationships between features (e.g., $x_0 \\cdot x_1$).\n","\n","- **Higher-order nonlinear term**  \n","  Introduces nonlinearity (e.g., $x_0^2$) to model more complex decision surfaces.\n","\n","We compare models based on predictive accuracy, uncertainty quantification, and calibration to evaluate how structural misspecification influences inference.\n","\n","---\n","\n","##  **Scenario 2: Misspecified Error Distributions**\n","\n","In this scenario, we fix the model structure and vary the error distribution in the data-generating process to simulate noise misspecification. The model is always fit using a logistic (Gumbel-distributed error) likelihood, but the true data-generating errors differ.\n","\n","###  Error distributions considered:\n","\n","- **Normal (probit-like behavior)**  \n","  Well-behaved and symmetric.\n","\n","- **Student's t (df=3)**  \n","  Heavy-tailed distribution, capturing extreme values.\n","\n","- **Cauchy**  \n","  Extremely heavy-tailed with undefined moments — models extreme outliers.\n","\n","- **Normal-contaminated**  \n","  Mixture of 90% standard normal and 10% noise with large variance (e.g., $\\mathcal{N}(0, 25)$).\n","\n","By fitting the same model to data with these different noise structures, we assess robustness of posterior inference, calibration, and classification accuracy under error term misspecification.\n","\n","---\n","\n","Each scenario is designed to isolate specific sources of model misspecification and test the effectiveness of Bayesian and Generalized Bayesian approaches under controlled deviations from the ideal model assumptions.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YiXRnjtqpCdt"},"source":["##  Experimental Setup\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6g2sPQmDpGDD"},"source":["###  True Data Generating Process (DGP)\n","\n","The data is generated using a utility model that includes **all three structural components**:\n","\n","$u_i = X_i \\beta_{g[i]} + \\gamma_1 (x_{i0} \\cdot x_{i1}) + \\gamma_2 x_{i0}^2 + z_i, \\quad z_i \\sim \\mathcal{N}(0, 1)\n","$, $\n","y_i = \\mathbb{1}(u_i > 0)\n","$\n","\n","- $X_i$: observed features\n","- $\\beta_{g[i]}$: group-specific coefficients\n","- $z_i$: latent noise\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SOFmNxl0pJue"},"source":["###  Model Variants\n","\n","Each model begins from the same **baseline model (M0)** and adds only one component at a time:\n","\n","| Model Name | Included Components |\n","|------------|---------------------|\n","| **M0** (Baseline) | Linear terms only, no group structure |\n","| **M1** | M0 + Group-level effect |\n","| **M2** | M0 + Interaction term $(x_0 \\cdot x_1)$ |\n","| **M3** | M0 + Nonlinear term $(x_0^2)$ |\n","| **Mf** | Model with every term |\n","\n","This structure allows us to isolate the effect of each component.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"9KWPwvqip6f7"},"source":["##  Methodology\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3Bgz8lhMp8ez"},"source":["###  Dataset Description\n","\n","The dataset simulates a realistic marketing scenario where customers are exposed to targeted campaigns. Each row corresponds to an individual customer and includes both behavioral features and treatment indicators.\n","\n","| Variable               | Description                                                                 |\n","|------------------------|-----------------------------------------------------------------------------|\n","| `group_id` / `group_label` | Customer tier (Bronze, Silver, Gold, Platinum, VIP)                      |\n","| `logins_last_week`     | Number of logins in the past 7 days (indicates user activity)              |\n","| `previous_purchases`   | Number of purchases in the past 30 days (baseline interest)                |\n","| `viewed_target_category` | Whether the customer viewed items in the target category (binary: 0/1)  |\n","| `discount_received`    | Whether the customer received a discount (binary treatment: 0/1)           |\n","| `y`                    | Binary purchase response (1 = purchase, 0 = no purchase)                   |\n"]},{"cell_type":"markdown","metadata":{"id":"gKc8QjNfp-US"},"source":["###  Model Implementation\n","\n","- Use **PyMC** with **Bayesian inference (NUTS)** for all models.\n","- M0: shared β coefficients.\n","- M1: group-level β.\n","- M2: add interaction term to features.\n","- M3: add squared term to features.\n","- Mf: add all term to features.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_BpNviWhqAjE"},"source":["###  Inference Procedure\n","\n","- Sample posterior using MCMC.\n","- Compute posterior predictive probabilities for test set.\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SxU2f90sqCCK"},"source":["##  Evaluation Metrics\n","\n","| Metric | Description |\n","|--------|-------------|\n","| **Accuracy** | Test set classification performance |\n","| **Brier Score** | Measures quality of probabilistic prediction |\n","| **Log Loss** | Measures quality of probabilistic prediction |\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1bQFPkZuqD9y"},"source":["##  Interpretation Goals\n","\n","- Which component provides the largest improvement over the baseline?\n","- Do some components lead to overconfidence or poor calibration?\n","- How does Quasi-Bayes inference compare in robustness under each misspecification?\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eaTMO6JQfFR"},"outputs":[],"source":["pip install pyro-ppl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCrb4y4fo3rt"},"outputs":[],"source":["# Import all packages\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.special import expit\n","import pymc as pm\n","import arviz as az\n","import matplotlib.pyplot as plt\n","from scipy.special import expit\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pytensor.tensor as pt\n","from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss, log_loss\n","from sklearn.preprocessing import StandardScaler\n","from scipy.stats import norm\n","import arviz as az\n","\n","import pyro\n","import pyro.infer.mcmc as pmcmc\n","from pyro.infer.mcmc import NUTS"]},{"cell_type":"markdown","metadata":{"id":"D14Y0r5Yrd3x"},"source":["# Data simulation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzRpLbBjrYx7"},"outputs":[],"source":["# Data simulation\n","# Set random seed for reproducibility\n","np.random.seed(50)\n","\n","# Configuration\n","group_labels = [\"Bronze\", \"Silver\", \"Gold\", \"Platinum\", \"VIP\"]\n","n_groups = len(group_labels)\n","n_per_group = 10000  # observations per group\n","\n","# Group-level multiplier: higher-tier customers are more responsive to discounts\n","group_effects = np.linspace(0.5, 2.0, n_groups)\n","\n","# Empty list to store simulated rows\n","rows = []\n","\n","for j in range(n_groups):\n","    group_name = group_labels[j]\n","    group_boost = group_effects[j]\n","\n","    for _ in range(n_per_group):\n","        # Features\n","        logins_last_week = np.random.poisson(lam=5)                   # user activeness\n","        previous_purchases = np.random.poisson(lam=2)                 # prior purchase behavior\n","        viewed_target_category = np.random.binomial(1, p=0.5)         # relevance exposure\n","        discount_received = np.random.binomial(1, p=0.5)              # marketing treatment\n","\n","        # True logit: combine all meaningful effects\n","        U = (\n","            -4.5\n","            + 0.1 * previous_purchases\n","            + 0.6 * viewed_target_category\n","            + 0.9 * discount_received\n","            + 3 * viewed_target_category * discount_received\n","            + 1.1 * group_boost * discount_received\n","            + 0.13 * previous_purchases**2\n","            + np.random.normal(0, 1)  # heavy-tailed noise\n","            )\n","\n","        y = 1 if U >0 else 0\n","\n","        # Store observation\n","        rows.append({\n","            \"group_id\": j,\n","            \"group_label\": group_name,\n","            \"logins_last_week\": logins_last_week,\n","            \"previous_purchases\": previous_purchases,\n","            \"viewed_target_category\": viewed_target_category,\n","            \"discount_received\": discount_received,\n","            \"y\": y\n","        })\n","\n","# Create DataFrame\n","df_simulated_test = pd.DataFrame(rows)\n","\n","df_simulated_test.head()\n","\n","print(df_simulated_test['y'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cstSfT5tCFLP"},"outputs":[],"source":["def generate_noise(noise_type, size):\n","    \"\"\"\n","    Generate random noise from a specified distribution.\n","\n","    Args:\n","        noise_type (str): Type of noise to generate. Supported values are:\n","            - \"normal\": Standard normal distribution N(0, 1).\n","            - \"t\": Student's t-distribution with 2 degrees of freedom.\n","            - \"cauchy\": Standard Cauchy distribution.\n","            - \"contaminated\": Contaminated normal distribution.\n","              Starts from N(0,1) and adds high-variance noise (N(0,5))\n","              to 10% of randomly selected samples.\n","        size (int): Number of samples to generate.\n","\n","    Returns:\n","        np.ndarray: Array of generated noise samples.\n","\n","    Raises:\n","        ValueError: If an unsupported noise_type is provided.\n","    \"\"\"\n","    if noise_type == \"normal\":\n","        return np.random.normal(0, 1, size=size)\n","    elif noise_type == \"t\":\n","        return np.random.standard_t(df=2, size=size)\n","    elif noise_type == \"cauchy\":\n","        return np.random.standard_cauchy(size=size)\n","    elif noise_type == \"contaminated\":\n","        base = np.random.normal(0, 1, size=size)\n","        outlier_idx = np.random.choice(size, size=int(0.1 * size), replace=False)\n","        base[outlier_idx] += np.random.normal(0, 5, size=len(outlier_idx))\n","        return base\n","    else:\n","        raise ValueError(\"Unsupported noise type\")\n","\n","\n","def simulate_dataset(noise_type,n_per_group):\n","    \"\"\"\n","    Simulate a synthetic customer-choice dataset with group heterogeneity,\n","    interactions, nonlinear effects, and noise.\n","\n","    Args:\n","        noise_type (str): Type of noise distribution used to generate latent utilities.\n","            Supported values are the same as in `generate_noise` (\"normal\", \"t\",\n","            \"cauchy\", \"contaminated\").\n","        n_per_group (int): Number of samples to generate per group.\n","\n","    Returns:\n","        pd.DataFrame: A standardized dataset with the following columns:\n","            - \"group_id\": Numeric ID of the group.\n","            - \"group_label\": Name of the group.\n","            - \"logins_last_week\": Scaled count of logins in the last week.\n","            - \"previous_purchases\": Scaled count of previous purchases.\n","            - \"viewed_target_category\": Binary indicator (1 if viewed target category).\n","            - \"discount_received\": Binary indicator (1 if discount was received).\n","            - \"y\": Binary outcome (1 if latent utility > 0, else 0).\n","\n","    \"\"\"\n","    rows = []\n","    noise_vector = generate_noise(noise_type, size=n_groups * n_per_group)\n","    noise_counter = 0\n","\n","    for j in range(n_groups):\n","        group_name = group_labels[j]\n","        group_boost = group_effects[j]\n","\n","        for _ in range(n_per_group):\n","            logins_last_week = np.random.poisson(lam=5)\n","            previous_purchases = np.random.poisson(lam=2)\n","            viewed_target_category = np.random.binomial(1, p=0.5)\n","            discount_received = np.random.binomial(1, p=0.5)\n","\n","            U = (\n","                -4.5\n","                + 0.1 * previous_purchases\n","                + 0.6 * viewed_target_category\n","                + 0.9 * discount_received\n","                + 3 * viewed_target_category * discount_received\n","                + 1.1 * group_boost * discount_received\n","                + 0.13 * previous_purchases**2\n","                + np.random.normal(0, 1)\n","                )\n","            noise_counter += 1\n","\n","\n","            y = 1 if U >0 else 0\n","\n","            rows.append({\n","                \"group_id\": j,\n","                \"group_label\": group_name,\n","                \"logins_last_week\": logins_last_week,\n","                \"previous_purchases\": previous_purchases,\n","                \"viewed_target_category\": viewed_target_category,\n","                \"discount_received\": discount_received,\n","                \"y\": y\n","            })\n","    df_simulated = pd.DataFrame(rows)\n","    scaler = StandardScaler()\n","    df_simulated[[\"logins_last_week\", \"previous_purchases\"]] = scaler.fit_transform(\n","    df_simulated[[\"logins_last_week\", \"previous_purchases\"]])\n","    return df_simulated\n","\n","\n","feature_cols = [\n","    \"logins_last_week\",\n","    \"previous_purchases\",\n","    \"viewed_target_category\",\n","    \"discount_received\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"841lIEWSs0Kr"},"source":["##  Why Normalize Features in Bayesian Hierarchical Models?\n","\n","Feature normalization helps ensure **stable inference and fair comparisons** in Bayesian and Quasi-Bayesian models.\n","\n","---\n","\n","###  Benefits of Normalization\n","\n","- **Faster, more stable sampling**  \n","  NUTS/HMC samplers perform better when all features are on a similar scale.\n","  \n","- **Better prior behavior**  \n","  Priors like `β ~ Normal(0, 1)` assume standardized inputs.\n","  \n","- **More interpretable posteriors**  \n","  Coefficients are easier to compare when features are normalized.\n","  \n","- **Consistent across models**  \n","  Helps maintain numerical stability in both simple and complex structures.\n","\n","---\n","\n","###  When to Normalize\n","\n","| Feature Type             | Normalize? |\n","|--------------------------|------------|\n","| Continuous (e.g. counts) |  Yes      |\n","| Binary or categorical    |  No       |\n","\n","---\n","\n","###  How to Normalize\n","\n","Use **z-score standardization**:\n","\n","$$\n","z = \\frac{x - \\mu}{\\sigma}\n","$$\n","\n","Apply to:\n","- `logins_last_week`\n","- `previous_purchases`\n","\n","Leave binary features like `viewed_target_category`, `discount_received` unchanged.\n","\n","---\n","\n","###  Note\n","\n","> While normalization may not change **relative model rankings**,  \n","> it improves **convergence, interpretability, and sampling efficiency** —  \n","> especially in hierarchical and Quasi-Bayesian models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHiIhmxctjHy"},"outputs":[],"source":["scaler = StandardScaler()\n","df_simulated_test[[\"logins_last_week\", \"previous_purchases\"]] = scaler.fit_transform(\n","    df_simulated_test[[\"logins_last_week\", \"previous_purchases\"]]\n",")\n","\n","feature_cols = [\n","    \"logins_last_week\",\n","    \"previous_purchases\",\n","    \"viewed_target_category\",\n","    \"discount_received\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"1e7S98FhvDuu"},"source":["# Scenario 1 : Utility misspecification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0NPGP5mtpVF"},"outputs":[],"source":["def phi(x):\n","    \"\"\"\n","    Standard normal cumulative distribution function (CDF).\n","\n","    Args:\n","        x (float or array-like): Input value(s).\n","\n","    Returns:\n","        float or array-like: The probability that a standard normal random\n","        variable is less than or equal to x.\n","    \"\"\"\n","    return 0.5 * (1 + pt.erf(x / pt.sqrt(2)))\n","\n","\n","def define_model(df, feature_cols, group_idx=None,\n","                 group=False, interaction=False, nonlinear=False):\n","    \"\"\"\n","    Construct a PyMC model skeleton (design matrix X, coefficients beta, and linear\n","    predictor eta) without attaching a likelihood. You can later add either a\n","    Bayesian likelihood (e.g., probit/logit) or plug in a Quasi-Bayesian\n","    objective.\n","\n","    Args:\n","        df (pd.DataFrame): Input dataset. Must contain columns in `feature_cols`,\n","            and always \"y\". If `group=True`, must also contain \"group_id\".\n","            If `interaction=True`, must contain\n","            \"viewed_target_category\" and \"discount_received\".\n","            If `nonlinear=True`, must contain \"previous_purchases\".\n","        feature_cols (list[str]): Base feature column names to include in X.\n","        group_idx (np.ndarray or None): Integer array of length N mapping each row\n","            to a group index in {0, …, G-1}. Required when `group=True`.\n","        group (bool): If True, use a hierarchical prior and group-specific betas\n","            with Normal(mu, sigma) per coefficient.\n","        interaction (bool): If True, add an extra column\n","            `interaction = viewed_target_category * discount_received`.\n","        nonlinear (bool): If True, add an extra column\n","            `purchases_squared = previous_purchases ** 2`.\n","\n","    Returns:\n","        tuple:\n","            - model (pm.Model): A PyMC model object with data container(s) and priors\n","              over coefficients but **no likelihood** attached.\n","            - eta (pt.TensorVariable): Linear predictor of shape (N,).\n","            - y (np.ndarray): Binary target array extracted from `df[\"y\"]]`.\n","    \"\"\"\n","    X = df[feature_cols].copy()\n","\n","    # Add interaction term if enabled\n","    if interaction:\n","        X[\"interaction\"] = df[\"viewed_target_category\"] * df[\"discount_received\"]\n","\n","    # Add nonlinear term if enabled\n","    if nonlinear:\n","        X[\"purchases_squared\"] = df[\"previous_purchases\"] ** 2  # assume preprocessed if needed\n","\n","    X_np = X.values.astype(\"float64\")\n","    y = df[\"y\"].values\n","    N, K = X.shape\n","\n","    with pm.Model() as model:\n","        X_shared = pm.Data(\"X\", X_np)\n","        if group:\n","            mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=K)\n","            sigma = pm.Exponential(\"sigma\", lam=1.0, shape=K)\n","            beta = pm.Normal(\"beta\", mu=mu, sigma=sigma, shape=(df[\"group_id\"].nunique(), K))\n","            eta = pt.sum(X_shared * beta[group_idx], axis=1)\n","        else:\n","            beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=K)\n","            eta = pt.dot(X_shared, beta)\n","\n","    return model, eta, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWNILLGsvJfe"},"outputs":[],"source":["def run_bayesian_model(model, eta, y, link=\"probit\", **sample_kwargs):\n","    \"\"\"\n","    Attach a Bernoulli likelihood to the provided linear predictor and run MCMC\n","    sampling in PyMC.\n","\n","    Args:\n","        model (pm.Model): A PyMC model (typically from `define_model`) that already\n","            defines the linear predictor `eta` and any priors but has no likelihood.\n","        eta (pt.TensorVariable): Linear predictor of shape (N,). Typically aano/pytensor\n","            tensor built from the design matrix and coefficients.\n","        y (array-like): Binary observations in {0, 1}, length N.\n","        link (str, optional): Link function to map `eta` to probabilities.\n","            - \"probit\": uses Φ(eta) via `phi`.\n","            - \"logit\": uses logistic σ(eta).\n","            Defaults to \"probit\".\n","        **sample_kwargs: Extra keyword arguments passed directly to `pm.sample`\n","            (e.g., draws, tune, target_accept, chains, cores, random_seed).\n","\n","    Returns:\n","        arviz.InferenceData: Posterior samples returned by `pm.sample`.\n","\n","    \"\"\"\n","    with model:\n","      eps = 1e-6\n","\n","      if link == \"probit\":\n","          p_raw = phi(eta)\n","      elif link == \"logit\":\n","          p_raw = pm.math.sigmoid(eta)\n","\n","      p = pm.Deterministic(\"p\", pm.math.clip(p_raw, eps, 1 - eps))\n","      pm.Bernoulli(\"y_obs\", p=p, observed=y)\n","      trace = pm.sample(**sample_kwargs)\n","    return trace\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2i6doMVvWi3"},"outputs":[],"source":["def loss_fn(y_true, y_pred, kind,alpha = None):\n","    \"\"\"\n","    Compute different loss functions for binary outcomes.\n","\n","    Args:\n","        y_true (array-like): Ground-truth binary labels (0 or 1).\n","        y_pred (array-like): Predicted probabilities in (0, 1). Values are\n","            clipped to [1e-6, 1 - 1e-6] for numerical stability.\n","        kind (str): Type of loss function. Supported options:\n","            - \"bce\": Binary cross-entropy loss\n","                L = - Σ [ y log(p) + (1 - y) log(1 - p) ]\n","            - \"squared\": Squared error loss\n","                L = Σ (y - p)^2\n","            - \"huber\": Huber loss (δ = 1.0 fixed)\n","                Smooth transition between squared error (small residuals)\n","                and absolute error (large residuals).\n","            - \"sph\": Scaled pseudo-Huber loss (requires `alpha` > 0)\n","                ℓ_SPH,α(t) = α √(1 + α²) ( √(1 + (t/α)²) - 1 )\n","        alpha (float, optional): Scaling parameter required only for \"sph\".\n","\n","    Returns:\n","        pt.TensorVariable: Scalar tensor representing the total loss\n","        (sum over all samples).\n","    \"\"\"\n","    p = pt.clip(y_pred, 1e-6, 1 - 1e-6)\n","    if kind == \"bce\":\n","        return -pt.sum(y_true * pt.log(p) + (1 - y_true) *pt.log(1 - p))\n","    elif kind == \"squared\":\n","        return pt.sum(pt.sqr(y_true - p))\n","    elif kind == \"huber\":\n","        delta = 1.0\n","        residual = y_true - p\n","        return pt.sum(pt.switch(pt.abs(residual) <= delta,\n","                                0.5 * residual**2,\n","                                delta * (pt.abs(residual) - 0.5 * delta)))\n","    elif kind == \"sph\":\n","        t = y_true - p\n","        scale = alpha * pt.sqrt(1.0 + alpha**2)\n","        loss_i = scale * (pt.sqrt(1.0 + (t / alpha)**2) - 1.0)\n","        return pt.sum(loss_i)\n","    else:\n","        raise ValueError(\"Unknown loss kind\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klHeATNF6rA2"},"outputs":[],"source":["def run_quasi_model(model, eta, y, loss_kind=\"bce\", **sample_kwargs):\n","    \"\"\"\n","    Attach a loss-based (Quasi-Bayesian) objective to a PyMC model\n","    and run sampling. The likelihood is replaced by a `Potential` equal to the\n","    negative loss, i.e., log-posterior ∝ prior − loss(y, p).\n","\n","    Args:\n","        model (pm.Model): A PyMC model (e.g., from `define_model`) that defines\n","            a linear predictor `eta` and priors but has no likelihood attached.\n","        eta (pt.TensorVariable): Linear predictor of shape (N,).\n","        y (array-like): Binary targets in {0, 1}, length N.\n","        loss_kind (str, optional): Loss to use for the quasi objective. One of:\n","            - \"bce\": Binary cross-entropy.\n","            - \"squared\": Squared error.\n","            - \"huber\": Huber (δ = 1).\n","            - \"sph\": Scaled pseudo-Huber (introduces α > 0).\n","        **sample_kwargs: Extra arguments forwarded to `pm.sample`\n","            (e.g., draws, tune, target_accept, chains, cores, random_seed).\n","\n","    Returns:\n","        arviz.InferenceData: Posterior samples under the chosen quasi objective.\n","\n","    \"\"\"\n","    with model:\n","        p = pm.Deterministic(\"p\", phi(eta))\n","\n","        step_kwargs = {}\n","        alpha_sq_rv = None\n","        alpha_det = None\n","\n","        if loss_kind == \"sph\":\n","            alpha_sq_rv = pm.Gamma(\"alpha_sq\", alpha=1.0, beta=1.0)\n","            alpha_det = pm.Deterministic(\"alpha\", pt.sqrt(alpha_sq_rv))\n","            loss = loss_fn(y, p, \"sph\", alpha=alpha_det)\n","            step_kwargs[\"step\"] = [pm.Slice(vars=[alpha_sq_rv])]\n","        else:\n","            loss = loss_fn(y, p, loss_kind)\n","\n","        pm.Potential(f\"{loss_kind}_loss\", -loss)\n","\n","        trace = pm.sample(**{**step_kwargs, **sample_kwargs})\n","\n","    return trace"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmUv89QHGDLC"},"outputs":[],"source":["def run_experiment_loop(\n","    seeds,\n","    feature_cols,\n","    noise_type_for_train=\"normal\",\n","    npergroup=200,\n","    group=False, interaction=False, nonlinear=False,\n","    use_quasi=False, loss_kind=\"bce\",\n","    draws=1000, tune=1000, target_accept=0.9):\n","    results = []\n","    \"\"\"\n","    Run repeated simulate–fit–evaluate cycles over multiple random seeds.\n","\n","    Workflow:\n","        For each seed:\n","          1) Simulate a training set (size = n_groups * npergroup) and a large\n","             test set (fixed at 10,000) from the specified noise type.\n","          2) Build a PyMC model skeleton via `define_model` with optional\n","             group heterogeneity, interaction, and nonlinear terms.\n","          3) Fit either:\n","               - Bayesian model with Bernoulli likelihood (probit/logit), or\n","               - Quasi-/Generalized-Bayesian model using a loss via `pm.Potential`.\n","          4) Compute posterior predictive probabilities on the test set by\n","             averaging Φ(Xβ) across all posterior β samples (probit link).\n","          5) Report Accuracy, Log-loss, and Brier score.\n","\n","    Args:\n","        seeds (Iterable[int]): Random seeds to iterate over; one experiment per seed.\n","        feature_cols (list[str]): Base feature names used to build X.\n","        noise_type_for_train (str, optional): Noise family for simulation\n","            (\"normal\", \"t\", \"cauchy\", \"contaminated\"). Defaults to \"normal\".\n","        npergroup (int, optional): Number of training samples per group. Defaults to 200.\n","        group (bool, optional): If True, fit a hierarchical model with group-specific betas.\n","        interaction (bool, optional): If True, add X[:, \"interaction\"] =\n","            viewed_target_category * discount_received.\n","        nonlinear (bool, optional): If True, add X[:, \"purchases_squared\"] =\n","            previous_purchases ** 2.\n","        use_quasi (bool, optional): If True, fit the quasi model; otherwise fit the\n","            standard Bayesian likelihood model.\n","        loss_kind (str, optional): Loss name when `use_quasi=True`\n","            (\"bce\", \"squared\", \"huber\", \"sph\"). Ignored for Bayesian mode.\n","        draws (int, optional): Number of MCMC draws for `pm.sample`. Defaults to 1000.\n","        tune (int, optional): Number of tuning steps for `pm.sample`. Defaults to 1000.\n","        target_accept (float, optional): Target acceptance rate for NUTS. Defaults to 0.9.\n","\n","    Returns:\n","        pd.DataFrame: One row per seed with columns:\n","            - \"seed\": Seed value used.\n","            - \"acc\": Accuracy on the test set using threshold 0.5 on mean p.\n","            - \"logloss\": Log-loss computed on mean posterior predictive probs.\n","            - \"brier\": Brier score on mean posterior predictive probs.\n","            - \"model_type\": \"quasi\" or \"bayes\".\n","            - \"loss_kind\": Loss name when quasi is used; otherwise None.\n","\n","    \"\"\"\n","\n","    for seed in seeds:\n","        np.random.seed(seed)\n","\n","        df_train = simulate_dataset(noise_type_for_train,npergroup)\n","        group_idx_train = df_train[\"group_id\"].values if group else None\n","\n","        df_test = simulate_dataset(noise_type_for_train,10000)\n","        X_test = df_test[feature_cols].copy()\n","        y_test = df_test[\"y\"].values\n","        group_idx_test = df_test[\"group_id\"].values if group else None\n","        if interaction:\n","          X_test[\"interaction\"] = X_test[\"viewed_target_category\"] * X_test[\"discount_received\"]\n","        if nonlinear:\n","          X_test[\"purchases_squared\"] = X_test[\"previous_purchases\"] ** 2\n","        X_test = X_test.values.astype(\"float64\")\n","\n","\n","        model, eta, y = define_model(\n","            df_train, feature_cols,\n","            group_idx=group_idx_train,\n","            group=group, interaction=interaction, nonlinear=nonlinear\n","        )\n","\n","        if use_quasi:\n","            trace = run_quasi_model(\n","                model, eta, y,\n","                loss_kind=loss_kind,\n","                draws=draws, tune=tune, target_accept=target_accept,\n","                return_inferencedata=True,     idata_kwargs={\"log_likelihood\": True}\n","            )\n","        else:\n","            trace = run_bayesian_model(\n","                model, eta, y,\n","                draws=draws, tune=tune, target_accept=target_accept,\n","                return_inferencedata=True,     idata_kwargs={\"log_likelihood\": True}\n","            )\n","\n","\n","        beta_da = trace.posterior[\"beta\"].stack(sample=(\"chain\", \"draw\"))\n","\n","        if beta_da.ndim == 2:\n","          beta = beta_da.transpose(\"sample\", ...).values\n","\n","          eta = X_test @ beta.T\n","\n","        else:\n","          beta = beta_da.transpose(\"sample\", ...).values\n","          S, G, K = beta.shape\n","\n","\n","          beta_g = beta[:, group_idx_test, :]\n","\n","          eta = np.einsum(\"snk,nk->ns\", beta_g, X_test)\n","\n","        p_samples = norm.cdf(eta)\n","\n","\n","        p_mean = p_samples.mean(axis=1)\n","\n","        y_pred = (p_mean >= 0.5).astype(int)\n","\n","        acc = accuracy_score(y_test, y_pred)\n","        logloss_val = log_loss(y_test, p_mean, labels=[0,1])\n","        brier = brier_score_loss(y_test, p_mean)\n","\n","        results.append({\n","            \"seed\": seed,\n","            \"acc\": acc,\n","            \"logloss\": logloss_val,\n","            \"brier\": brier,\n","            \"model_type\": \"quasi\" if use_quasi else \"bayes\",\n","            \"loss_kind\": loss_kind if use_quasi else None\n","        })\n","\n","    return pd.DataFrame(results)"]},{"cell_type":"markdown","metadata":{"id":"F7V5ylfxQHso"},"source":["## 1. Classical Bayesian model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDYyagEkGAMe"},"outputs":[],"source":["df_results_bayes_m0 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=False,\n","    nonlinear=False,\n","    use_quasi=False\n",")\n","\n","df_results_bayes_m1 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=True,\n","    interaction=False,\n","    nonlinear=False,\n","    use_quasi=False\n",")\n","\n","df_results_bayes_m2 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=True,\n","    nonlinear=False,\n","    use_quasi=False\n",")\n","\n","df_results_bayes_m3 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=False,\n","    nonlinear=True,\n","    use_quasi=False\n",")\n","df_results_bayes_mf = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=True,\n","    interaction=True,\n","    nonlinear=True,\n","    use_quasi=False\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEZFmIovBh8N"},"outputs":[],"source":["print(df_results_bayes_m0)\n","print(df_results_bayes_m1)\n","print(df_results_bayes_m2)\n","print(df_results_bayes_m3)\n","print(df_results_bayes_mf)"]},{"cell_type":"markdown","metadata":{"id":"6-zta4yqQYSU"},"source":["## 2. Quasi Bayes model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1ctvpXjxLLu"},"outputs":[],"source":["df_results_quasi_m0 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=False,\n","    nonlinear=False,\n","    use_quasi=True\n",")\n","\n","print(df_results_quasi_m0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gthRWP9lZCYP"},"outputs":[],"source":["df_results_quasi_m1 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=True,\n","    interaction=False,\n","    nonlinear=False,\n","    use_quasi=True\n",")\n","\n","print(df_results_quasi_m1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cWHj_4oZCDL"},"outputs":[],"source":["df_results_quasi_m2 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=True,\n","    nonlinear=False,\n","    use_quasi=True\n",")\n","\n","print(df_results_quasi_m2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHyoNLDyZB3h"},"outputs":[],"source":["df_results_quasi_m3 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=False,\n","    nonlinear=True,\n","    use_quasi=True\n",")\n","print(df_results_quasi_m3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAewYFHLD_es"},"outputs":[],"source":["df_results_quasi_mf = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=True,\n","    interaction=True,\n","    nonlinear=True,\n","    use_quasi=True\n",")\n","\n","print(df_results_quasi_mf)\n"]},{"cell_type":"markdown","metadata":{"id":"P0t-Z8wOQiNM"},"source":["## 3. SPH Bayes model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NryI8XoFx1P-"},"outputs":[],"source":["df_results_sph_m0 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=False,\n","    nonlinear=False,\n","    use_quasi=True,loss_kind = \"sph\"\n",")\n","\n","print(df_results_sph_m0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hN6K4uUuaZT9"},"outputs":[],"source":["df_results_sph_m1 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=True,\n","    interaction=False,\n","    nonlinear=False,\n","    use_quasi=True,loss_kind = \"sph\"\n",")\n","\n","print(df_results_sph_m1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5IX1Sic0aZO5"},"outputs":[],"source":["df_results_sph_m2 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=True,\n","    nonlinear=False,\n","    use_quasi=True,loss_kind = \"sph\"\n",")\n","\n","print(df_results_sph_m2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fjlv1ZNDaZHQ"},"outputs":[],"source":["df_results_sph_m3 = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=False,\n","    interaction=False,\n","    nonlinear=True,\n","    use_quasi=True,loss_kind = \"sph\"\n",")\n","print(df_results_sph_m3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXJf5qKFEE5b"},"outputs":[],"source":["df_results_sph_mf = run_experiment_loop(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","    group=True,\n","    interaction=True,\n","    nonlinear=True,\n","    use_quasi=True,loss_kind = \"sph\"\n",")\n","\n","print(df_results_sph_mf)"]},{"cell_type":"markdown","metadata":{"id":"TeSCoXCvQm-k"},"source":["## 4. KSD Bayes model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-sJWxxvxayd"},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from typing import Tuple, Optional, Dict\n","\n","def build_features(\n","    df: pd.DataFrame,\n","    feature_cols,\n","    interaction: bool,\n","    nonlinear: bool,\n","    group: bool\n",") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, int, int]:\n","    \"\"\"\n","    Build PyTorch-ready feature tensors (with bias), target, group indices, and\n","    optional engineered features (interaction, nonlinear) from a pandas DataFrame.\n","\n","    Args:\n","        df (pd.DataFrame): Input data containing at least `feature_cols`, \"y\",\n","            and, if `group=True`, \"group_id\". For engineered features:\n","            - If `interaction=True`, expects \"viewed_target_category\" and \"discount_received\".\n","            - If `nonlinear=True`, expects \"previous_purchases\".\n","        feature_cols (Iterable[str]): Base feature names to include. A constant\n","            bias term (column of 1.0) is appended automatically.\n","        interaction (bool): If True, compute an extra column:\n","            (viewed_target_category * discount_received).\n","        nonlinear (bool): If True, compute an extra column:\n","            (previous_purchases ** 2).\n","        group (bool): If True, return integer group codes derived from \"group_id\"\n","            and set G to the number of unique groups; otherwise a zero vector and G=1.\n","\n","    Returns:\n","        Tuple[\n","            torch.Tensor,  # X: (N, D) float32, base features + bias\n","            torch.Tensor,  # y: (N,) float32, binary targets in {0., 1.}\n","            torch.Tensor,  # group_ids: (N,) int64 (long), group codes (or zeros if group=False)\n","            torch.Tensor,  # extra: (N, 2) float32, [interaction, nonlinear] (zeros for disabled parts)\n","            int,           # D: number of columns in X (including bias)\n","            int            # G: number of groups (>=1)\n","        ]\n","\n","    \"\"\"\n","    X_df = df[list(feature_cols)].copy()\n","    X_df[\"bias\"] = 1.0\n","    X = torch.tensor(X_df.values, dtype=torch.float32)\n","\n","    y = torch.tensor(df[\"y\"].values, dtype=torch.float32)\n","    if group:\n","        gids_raw = pd.Categorical(df[\"group_id\"])\n","        group_ids = torch.tensor(gids_raw.codes, dtype=torch.long)\n","        G = len(gids_raw.categories)\n","    else:\n","        group_ids = torch.zeros(len(df), dtype=torch.long)\n","        G = 1\n","\n","    inter = torch.tensor(\n","        (df[\"viewed_target_category\"] * df[\"discount_received\"]).values,\n","        dtype=torch.float32\n","    ).unsqueeze(1) if interaction else torch.zeros((len(df), 1), dtype=torch.float32)\n","\n","    nonlin = torch.tensor(\n","        (df[\"previous_purchases\"] ** 2).values,\n","        dtype=torch.float32\n","    ).unsqueeze(1) if nonlinear else torch.zeros((len(df), 1), dtype=torch.float32)\n","\n","    extra = torch.cat([inter, nonlin], dim=1)\n","    D = X.shape[1]\n","    return X, y, group_ids, extra, D, G\n","\n","\n","def unpack_theta(theta: torch.Tensor, D: int, G: int, use_gamma1: bool, use_gamma2: bool):\n","    \"\"\"\n","    Unpack a flat parameter vector into structured components:\n","    group-specific betas and optional scalars (gamma1, gamma2).\n","\n","    Args:\n","        theta (torch.Tensor): Flat parameter vector (shape: (G*D [+ 1 if use_gamma1] [+ 1 if use_gamma2],)).\n","        D (int): Number of base features in X (including bias if present).\n","        G (int): Number of groups (>=1).\n","        use_gamma1 (bool): Whether to include and return gamma1 (interaction weight).\n","        use_gamma2 (bool): Whether to include and return gamma2 (nonlinear weight).\n","\n","    Returns:\n","        dict: {\n","            \"betas\":  torch.Tensor of shape (G, D),\n","            \"gamma1\": scalar tensor (0.0 if not used),\n","            \"gamma2\": scalar tensor (0.0 if not used),\n","        }\n","    \"\"\"\n","    idx = 0\n","    if G > 1:\n","        betas = theta[idx: idx + G * D].reshape(G, D)\n","        idx += G * D\n","    else:\n","        betas = theta[idx: idx + D].unsqueeze(0)\n","        idx += D\n","\n","    gamma1 = theta[idx] if use_gamma1 else torch.tensor(0.0, dtype=theta.dtype, device=theta.device)\n","    idx += 1 if use_gamma1 else 0\n","\n","    gamma2 = theta[idx] if use_gamma2 else torch.tensor(0.0, dtype=theta.dtype, device=theta.device)\n","    idx += 1 if use_gamma2 else 0\n","\n","    return {\"betas\": betas, \"gamma1\": gamma1, \"gamma2\": gamma2}\n","\n","\n","def eta_fn(theta: torch.Tensor, X: torch.Tensor, group_ids: torch.Tensor,\n","           extra: torch.Tensor, D: int, G: int,\n","           use_gamma1: bool, use_gamma2: bool) -> torch.Tensor:\n","    \"\"\"\n","    Compute the linear predictor η(x) for a (possibly) group-heterogeneous model:\n","        η_i = x_i^T β_{g[i]} + γ1 * interaction_i + γ2 * nonlinear_i\n","\n","    Args:\n","        theta (torch.Tensor): Flat parameter vector compatible with `unpack_theta`.\n","        X (torch.Tensor): Design matrix of shape (N, D) matching the betas.\n","        group_ids (torch.Tensor): Long tensor of shape (N,) with entries in {0, …, G-1}.\n","        extra (torch.Tensor): Tensor of shape (N, 2) where:\n","            - extra[:, 0] = interaction feature (or zeros if disabled),\n","            - extra[:, 1] = nonlinear feature (or zeros if disabled).\n","        D (int): Number of base features (columns of X).\n","        G (int): Number of groups.\n","        use_gamma1 (bool): If True, include γ1 * extra[:, 0].\n","        use_gamma2 (bool): If True, include γ2 * extra[:, 1].\n","\n","    Returns:\n","        torch.Tensor: Linear predictor η of shape (N,), dtype/device following `X`.\n","\n","    \"\"\"\n","    pars = unpack_theta(theta, D, G, use_gamma1, use_gamma2)\n","    betas = pars[\"betas\"]\n","    xb = (X * betas[group_ids]).sum(dim=1)\n","    eta = xb\n","    if use_gamma1:\n","        eta = eta + pars[\"gamma1\"] * extra[:, 0]\n","    if use_gamma2:\n","        eta = eta + pars[\"gamma2\"] * extra[:, 1]\n","    return eta\n","\n","\n","SQRT2 = 1.4142135623730951\n","def norm_cdf(z: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Standard normal cumulative distribution function Φ(z) applied elementwise.\n","\n","    Args:\n","        z (torch.Tensor): Input tensor.\n","\n","    Returns:\n","        torch.Tensor: Elementwise CDF values in (0, 1).\n","    \"\"\"\n","    return 0.5 * (1.0 + torch.erf(z / SQRT2))\n","\n","def norm_ppf(u: torch.Tensor,eps: float = 1e-6) -> torch.Tensor:\n","    \"\"\"\n","    Standard normal percent-point function (inverse CDF) Φ^{-1}(u) applied elementwise.\n","\n","    Args:\n","        u (torch.Tensor): Probabilities. Values are clamped to [eps, 1 - eps]\n","            for numerical stability.\n","        eps (float, optional): Clamp margin to avoid infinities. Defaults to 1e-6.\n","\n","    Returns:\n","        torch.Tensor: Elementwise quantiles.\n","    \"\"\"\n","    u = u.clamp(eps, 1.0 - eps)\n","    return SQRT2 * torch.erfinv(2.0 * u - 1.0)\n","\n","@torch.no_grad()\n","def sample_truncnorm_from_probit(eta: torch.Tensor, y: torch.Tensor,eps: float = 1e-6) -> torch.Tensor:\n","    \"\"\"\n","    Draw latent Gaussian utilities z from the probit data augmentation\n","    (Albert–Chib) scheme:\n","        z | (y=1, η) ~ N(η, 1) truncated to (0, ∞)\n","        z | (y=0, η) ~ N(η, 1) truncated to (-∞, 0]\n","\n","    Uses inverse-transform sampling with numerically stable clamping.\n","\n","    Args:\n","        eta (torch.Tensor): Linear predictor η of shape (N,).\n","        y (torch.Tensor): Binary outcomes in {0, 1} (shape (N,)). Values > 0.5\n","            are treated as 1.\n","        eps (float, optional): Clamp margin for CDF/PPF to avoid 0/1. Defaults to 1e-6.\n","\n","    Returns:\n","        torch.Tensor: Latent samples z of shape (N,).\n","    \"\"\"\n","    N = eta.shape[0]\n","    a = (0.0 - eta)\n","    Phi_a = norm_cdf(a).clamp(eps, 1.0 - eps)\n","\n","    u1 = Phi_a + (1.0 - Phi_a) * torch.rand(N, device=eta.device)\n","    z1 = eta + norm_ppf(u1)\n","\n","    u0 = torch.rand(N, device=eta.device) * Phi_a\n","    z0 = eta + norm_ppf(u0)\n","\n","    return torch.where(y > 0.5, z1, z0)\n","\n","def _poly_stats(r: torch.Tensor, gamma: float):\n","    \"\"\"\n","    Utility for polynomial kernel derivatives with respect to r = ||x - x'||^2 (or a scalar proxy).\n","    Args:\n","        r (torch.Tensor): Nonnegative tensor (often pairwise squared distances).\n","        gamma (float): Polynomial kernel exponent γ.\n","\n","    Returns:\n","        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","\n","    k   = (1.0 + r).pow(gamma)\n","    kp  = gamma * (1.0 + r).pow(gamma - 1.0)\n","    kpp = gamma * (gamma - 1.0) * (1.0 + r).pow(gamma - 2.0)\n","    return k, kp, kpp\n","\n","def imq_cache_from_u(u: torch.Tensor, c: float = None, beta: float = 0.5):\n","    \"\"\"\n","    Build cached tensors for the IMQ (Inverse Multiquadric) kernel and its\n","    r-derivatives for a vector of scalars u (e.g., latent utilities).\n","\n","    Args:\n","        u (torch.Tensor): 1D or (N,1) tensor; will be reshaped to (N,1).\n","        c (float, optional): IMQ scale parameter. If None, uses the median\n","            heuristic on pairwise distances of u (once per call).\n","        beta (float, optional): IMQ exponent β. Defaults to 0.5.\n","\n","    Returns:\n","        dict: Cached components:\n","            - \"k\":      (N,N) kernel matrix (A^(-β))\n","            - \"kp\":     (N,N) first derivative wrt A ( -β A^(-β-1) )\n","            - \"dA_du\":  (N,N) ∂A/∂u   =  2 (u - u')\n","            - \"dA_dup\": (N,N) ∂A/∂u'  = -2 (u - u')\n","            - \"trace_base\": (N,N) base term for mixed second derivatives:\n","                  k'' * (∂A/∂u)(∂A/∂u') + k' * (∂²A/∂u∂u')\n","                  where ∂²A/∂u∂u' = -2\n","            - \"ones_col\": (N,1) column of ones (utility for reductions)\n","            - \"ones_row\": (1,N) row of ones\n","            - \"c\": chosen scale (float)\n","            - \"beta\": IMQ exponent (float)\n","    \"\"\"\n","    u = u.view(-1, 1)\n","    device = u.device\n","    N = u.shape[0]\n","    diff = u - u.t()\n","    r2 = diff.pow(2)\n","\n","    if c is None:\n","        tri = r2[torch.triu(torch.ones_like(r2, dtype=torch.bool), diagonal=1)]\n","        med = torch.sqrt(tri.median()).clamp_min(1e-8)\n","        c = med\n","\n","    A   = c*c + r2\n","    k   = A.pow(-beta)\n","    kp  = -beta * A.pow(-beta - 1.0)\n","    kpp = (-beta) * (-beta - 1.0) * A.pow(-beta - 2.0)\n","\n","    dA_du  =  2.0 * diff\n","    dA_dup = -2.0 * diff\n","\n","    trace_base = kpp * dA_du * dA_dup + kp * (-2.0)\n","\n","    ones_col = torch.ones(N, 1, device=device)\n","    ones_row = torch.ones(1, N, device=device)\n","\n","    return {\n","        \"k\": k, \"kp\": kp,\n","        \"dA_du\": dA_du, \"dA_dup\": dA_dup,\n","        \"trace_base\": trace_base,\n","        \"ones_col\": ones_col, \"ones_row\": ones_row,\n","        \"c\": c, \"beta\": beta\n","    }\n","\n","def ksd2_vstat_imq_cached(u: torch.Tensor, eta: torch.Tensor, cache: dict) -> torch.Tensor:\n","    \"\"\"\n","    Compute the V-statistic estimator of the squared Kernelized Stein Discrepancy (KSD^2)\n","    using a cached IMQ kernel and its derivatives.\n","\n","    Args:\n","        u (torch.Tensor): Latent samples (N,) or (N,1).\n","        eta (torch.Tensor): Linear predictor / score mean (N,) or (N,1).\n","        cache (dict): Precomputed kernel components from `imq_cache_from_u`, including:\n","            - \"k\": (N,N) kernel matrix\n","            - \"kp\": (N,N) derivative wrt A\n","            - \"dA_du\": (N,N) ∂A/∂u\n","            - \"dA_dup\": (N,N) ∂A/∂u'\n","            - \"trace_base\": (N,N) cross second derivative base term\n","            - \"ones_col\", \"ones_row\": helper tensors for broadcasting\n","\n","    Returns:\n","        torch.Tensor: Scalar tensor with the V-statistic estimate of KSD^2.\n","    \"\"\"\n","    u   = u.view(-1, 1)\n","    eta = eta.view(-1, 1)\n","    N   = u.shape[0]\n","    k         = cache[\"k\"]\n","    dA_du     = cache[\"dA_du\"]\n","    dA_dup    = cache[\"dA_dup\"]\n","    trace_base= cache[\"trace_base\"]\n","    ones_col  = cache[\"ones_col\"]\n","    ones_row  = cache[\"ones_row\"]\n","\n","\n","    s = -(u - eta)\n","    s_dot = s @ s.t()\n","\n","\n","    kp = cache[\"kp\"]\n","    grad_u_k  = kp * dA_du\n","    grad_up_k = kp * dA_dup\n","\n","    term1 = k * s_dot\n","    term2 = (s @ ones_row) * grad_up_k\n","    term3 = (ones_col @ s.t()) * grad_u_k\n","    Umat = term1 + term2 + term3 + trace_base\n","    return Umat.mean()\n","\n","def logpost_ksd_bayes(\n","    theta: torch.Tensor,\n","    X: torch.Tensor, y: torch.Tensor, group_ids: torch.Tensor,\n","    extra: torch.Tensor,\n","    D: int, G: int,\n","    use_gamma1: bool, use_gamma2: bool,\n","    u_imputed: torch.Tensor,\n","    beta_lr: float = 1.0,\n","    prior_std: float = 1.0,\n","    cache = None\n",") -> torch.Tensor:\n","    \"\"\"\n","    Log-posterior for KSD-Bayes with IMQ kernel.\n","\n","    Objective:\n","        log p(θ | data) ∝ log prior(θ) − β_lr * N * KSD²(u, η; θ)\n","\n","    Args:\n","        theta (torch.Tensor): Flat parameter vector (includes betas, gamma1/2).\n","        X (torch.Tensor): Feature matrix (N, D).\n","        y (torch.Tensor): Binary labels (N,). Not directly used here but kept\n","            for API consistency.\n","        group_ids (torch.Tensor): Group index per sample (N,).\n","        extra (torch.Tensor): Extra engineered features (interaction, nonlinear),\n","            shape (N, 2).\n","        D (int): Number of features in X.\n","        G (int): Number of groups.\n","        use_gamma1 (bool): Whether gamma1 is active in eta_fn.\n","        use_gamma2 (bool): Whether gamma2 is active in eta_fn.\n","        u_imputed (torch.Tensor): Latent variables sampled from truncated normals,\n","            used in Stein discrepancy.\n","        beta_lr (float, optional): Scaling factor (learning-rate like). Default = 1.0.\n","        prior_std (float, optional): Std dev of Normal(0, prior_std²) prior on θ.\n","            Default = 1.0.\n","        cache (dict, optional): Precomputed IMQ kernel cache from\n","            `imq_cache_from_u(u_imputed)`.\n","\n","    Returns:\n","        torch.Tensor: Scalar log-posterior (float32/float64, depending on inputs).\n","    \"\"\"\n","    eta = eta_fn(theta, X, group_ids, extra, D, G, use_gamma1, use_gamma2)\n","    ksd2 = ksd2_vstat_imq_cached(u_imputed, eta, cache)\n","    log_prior = -0.5 * theta.pow(2).sum() / (prior_std**2)\n","    N = X.shape[0]\n","    return log_prior - beta_lr * N * ksd2\n","\n","\n","\n","# @torch.no_grad()\n","\n","def make_potential_fn(\n","    X, y, group_ids, extra, D, G, use_gamma1, use_gamma2,\n","    u_imputed, beta_lr, prior_std, cache\n","):\n","    \"\"\"\n","    Wrap the KSD-Bayes log-posterior into a potential function compatible with\n","    gradient-free NUTS implementations (expects a dict with key \"theta\").\n","\n","    Returns:\n","        Callable[[dict], torch.Tensor]: A function that takes {\"theta\": θ}\n","        and returns the negative log-posterior (i.e., a potential/energy).\n","    \"\"\"\n","    def _potential(theta_dict):\n","        theta = theta_dict[\"theta\"]\n","        return -logpost_ksd_bayes(\n","            theta, X, y, group_ids, extra, D, G,\n","            use_gamma1, use_gamma2, u_imputed,\n","            beta_lr=beta_lr, prior_std=prior_std, cache=cache\n","        )\n","    return _potential\n","\n","def nuts_sample_theta(\n","    theta_init: torch.Tensor,\n","    X: torch.Tensor, y: torch.Tensor, group_ids: torch.Tensor, extra: torch.Tensor,\n","    D: int, G: int, use_gamma1: bool, use_gamma2: bool,\n","    u_imputed: torch.Tensor, cache: dict,\n","    beta_lr: float = 1.0, prior_std: float = 1.0,\n","    num_warmup: int = 300, num_samples: int = 100,\n","    target_accept_prob: float = 0.8, max_tree_depth: int = 10,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Run NUTS over θ using a potential defined by KSD-Bayes (negative log-posterior).\n","\n","    Args:\n","        theta_init (torch.Tensor): Initial parameter vector θ (flat).\n","        X, y, group_ids, extra, D, G, use_gamma1, use_gamma2:\n","            Model/data components passed through to `logpost_ksd_bayes`.\n","        u_imputed (torch.Tensor): Latent draws used inside the KSD term.\n","        cache (dict): IMQ kernel cache from `imq_cache_from_u(u_imputed)`.\n","        beta_lr (float): KSD scaling factor.\n","        prior_std (float): Std for N(0, prior_std²) prior on θ.\n","        num_warmup (int): NUTS warmup (adaptation) steps.\n","        num_samples (int): Number of post-warmup samples to draw.\n","        target_accept_prob (float): Target acceptance probability for NUTS.\n","        max_tree_depth (int): Maximum tree depth for NUTS.\n","\n","    Returns:\n","        torch.Tensor: Samples of θ with shape (num_samples, dim_theta) on the same\n","        device as `theta_init`.\n","    \"\"\"\n","    device = theta_init.device\n","    potential_fn = make_potential_fn(\n","        X, y, group_ids, extra, D, G, use_gamma1, use_gamma2,\n","        u_imputed, beta_lr, prior_std, cache\n","    )\n","\n","    kernel = NUTS(\n","        potential_fn=potential_fn,\n","        target_accept_prob=target_accept_prob,\n","        max_tree_depth=max_tree_depth,\n","        adapt_step_size=True,\n","        adapt_mass_matrix=True,\n","    )\n","\n","\n","    mcmc = None\n","    try:\n","        mcmc = pmcmc.MCMC(\n","            kernel,\n","            num_samples=num_samples,\n","            warmup_steps=num_warmup,\n","            initial_params={\"theta\": theta_init.detach()},\n","        )\n","        mcmc.run()\n","    except TypeError:\n","        mcmc = pmcmc.MCMC(\n","            kernel,\n","            num_samples=num_samples,\n","            initial_params={\"theta\": theta_init.detach()},\n","        )\n","        mcmc.run(num_warmup=num_warmup)\n","\n","    samples = mcmc.get_samples(group_by_chain=False)[\"theta\"].to(device)\n","    return samples\n","\n","def init_theta(D: int, G: int, use_gamma1: bool, use_gamma2: bool, scale: float = 0.1) -> torch.Tensor:\n","    \"\"\"\n","    Initialize a flat parameter vector θ for the (possibly) hierarchical model.\n","\n","    Layout size:\n","        - If G > 1: G * D coefficients (group-specific betas)\n","        - Else:     D coefficients (single beta)\n","        - +1 if use_gamma1 (interaction coefficient γ1)\n","        - +1 if use_gamma2 (nonlinear  coefficient γ2)\n","\n","    Args:\n","        D (int): Number of base features (including bias if present).\n","        G (int): Number of groups (>=1).\n","        use_gamma1 (bool): Include γ1 parameter if True.\n","        use_gamma2 (bool): Include γ2 parameter if True.\n","        scale (float, optional): Std dev for N(0, scale²) init. Defaults to 0.1.\n","\n","    Returns:\n","        torch.Tensor: θ ∈ R^k with k computed from the layout above.\n","    \"\"\"\n","    k = (G * D if G > 1 else D) + (1 if use_gamma1 else 0) + (1 if use_gamma2 else 0)\n","    return scale * torch.randn(k)\n","\n","def predict_probit(\n","    theta_mean: torch.Tensor,\n","    df_test: pd.DataFrame,\n","    feature_cols,\n","    interaction: bool, nonlinear: bool, group: bool\n",") -> Tuple[torch.Tensor, Dict[str, float]]:\n","    \"\"\"\n","    Posterior-predictive probabilities and metrics under a probit link using a\n","    single parameter vector (e.g., posterior mean of θ).\n","\n","    Steps:\n","      1) Build tensors from `df_test` (adds bias; optionally interaction/nonlinear).\n","      2) Compute η = xᵢᵀ β_{g[i]} + γ1 * interaction + γ2 * nonlinear.\n","      3) Map via probit: p = Φ(η).\n","      4) Return p and metrics (accuracy, Brier, log loss).\n","\n","    Args:\n","        theta_mean (torch.Tensor): Flat parameter vector θ (often posterior mean).\n","        df_test (pd.DataFrame): Test data.\n","        feature_cols (Iterable[str]): Base feature columns used in X.\n","        interaction (bool): If True, include interaction feature and γ1 term.\n","        nonlinear (bool): If True, include squared feature and γ2 term.\n","        group (bool): If True, use group-specific betas.\n","\n","    Returns:\n","        Tuple[torch.Tensor, Dict[str, float]]:\n","            - p: (N,) tensor of predicted probabilities.\n","            - metrics: {\"accuracy\": float, \"brier\": float, \"logloss\": float}.\n","    \"\"\"\n","    X, y, group_ids, extra, D, G = build_features(df_test, feature_cols, interaction, nonlinear, group)\n","\n","    device = theta_mean.device\n","    X = X.to(device)\n","    y = y.to(device)\n","    group_ids = group_ids.to(device)\n","    extra = extra.to(device)\n","    eta = eta_fn(theta_mean, X, group_ids, extra, D, G, interaction, nonlinear)\n","    p = 0.5 * (1.0 + torch.erf(eta / 1.4142135623730951))\n","    p = p.clamp(1e-8, 1-1e-8)\n","    yhat = (p > 0.5).float()\n","\n","    acc = (yhat == y).float().mean().item()\n","    brier = torch.mean((p - y) ** 2).item()\n","    logloss = -torch.mean(y * torch.log(p) + (1 - y) * torch.log(1 - p)).item()\n","\n","    return p, {\"accuracy\": acc, \"brier\": brier, \"logloss\": logloss}\n"]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from collections import deque\n","\n","def fit_ksd_bayes_nuts_ema_ensemble(\n","    df_train, df_test, feature_cols,\n","    interaction=True, nonlinear=True, group=True,\n","    n_outer=60,\n","    nuts_warmup=300, nuts_samples=30,\n","    frac_pp_mean=0.5,\n","    beta_lr=0.01, prior_std=1.0, imq_beta=0.5,\n","    target_accept_prob=0.90, max_tree_depth=10,\n","    alpha=0.25,\n","    K_ens=8,\n","    K_ma=7,\n","    tol_logloss=0.01, tol_brier=0.005, tol_acc=0.005,\n","    device=\"cuda\", verbose=True\n","):\n","    \"\"\"\n","    Fit KSD-Bayes parameters with an outer-loop EMA + inner NUTS sampler and\n","    ensemble posterior-predictive smoothing.\n","\n","    Procedure (per outer iteration t):\n","      1) Build tensors from `df_train` (optionally with interaction/nonlinear/group).\n","      2) Compute η = η(θ; X, group_ids, extra).\n","      3) Draw latent utilities u ~ TruncNormal(η, 1) using probit augmentation.\n","      4) Build IMQ kernel cache from u (median-heuristic scale).\n","      5) Run NUTS over θ with potential −logpost_ksd_bayes(θ; u, …) for\n","         `nuts_warmup` + `nuts_samples`.\n","      6) Compute inner mean θ̄ (over the `nuts_samples` draws) and update\n","         θ ← (1−α)·θ + α·θ̄   (EMA).\n","      7) Form p_mean_this_outer by averaging predictions over the last\n","         `frac_pp_mean` fraction of θ draws; push to a size-`K_ens` deque and\n","         compute ensemble p_ens as the mean across the deque.\n","      8) Track train metrics (accuracy, Brier, logloss). If the moving-average\n","         improvement over the last `K_ma` rounds is below tolerances\n","         (`tol_logloss`, `tol_brier`, `tol_acc`) for all three metrics, stop early.\n","\n","    After the loop:\n","      - Average predictions over **all** draws in the last NUTS run on `df_test`\n","        to produce test metrics.\n","\n","    Args:\n","        df_train (pd.DataFrame): Training data.\n","        df_test (pd.DataFrame): Test data.\n","        feature_cols (Iterable[str]): Base feature columns used to build X.\n","        interaction (bool): Include interaction feature and learn γ1.\n","        nonlinear (bool): Include squared feature and learn γ2.\n","        group (bool): Use group-specific betas (hierarchical).\n","        n_outer (int): Number of outer EMA iterations (max; may early-stop).\n","        nuts_warmup (int): NUTS warmup steps per outer iteration.\n","        nuts_samples (int): NUTS post-warmup samples per outer iteration.\n","        frac_pp_mean (float): Fraction in (0,1] of most recent θ draws used to\n","            compute per-outer posterior-predictive mean p.\n","        beta_lr (float): Scale β in the KSD-Bayes objective.\n","        prior_std (float): Std for N(0, prior_std²) prior on θ.\n","        imq_beta (float): IMQ kernel exponent β (>0).\n","        target_accept_prob (float): NUTS target acceptance probability.\n","        max_tree_depth (int): NUTS max tree depth.\n","        alpha (float): EMA mixing rate in (0,1].\n","        K_ens (int): Ensemble window size for p_ens over outer iterations.\n","        K_ma (int): Moving-average window for early-stopping comparisons.\n","        tol_logloss (float): Relative improvement threshold for logloss.\n","        tol_brier (float): Relative improvement threshold for Brier.\n","        tol_acc (float): Absolute improvement threshold for accuracy.\n","        device (str): Torch device for tensors (\"cuda\" or \"cpu\").\n","        verbose (bool): If True, print per-outer training metrics and early-stop info.\n","\n","    Returns:\n","        dict:\n","            - \"theta_path\": torch.Tensor, shape (T_eff, dim_θ)\n","                EMA θ snapshots over outer iterations (T_eff ≤ n_outer).\n","            - \"history_train\": dict with lists for \"logloss\", \"brier\", \"accuracy\".\n","            - \"stopped_at\": int or None, outer iteration index where early-stop occurred.\n","            - \"final_theta\": torch.Tensor, last EMA θ (on CPU).\n","            - \"metrics_test\": dict with {\"accuracy\", \"brier\", \"logloss\"} computed by\n","                averaging predictions over all θ draws from the final NUTS run.\n","    \"\"\"\n","\n","    X, y, group_ids, extra, D, G = build_features(df_train, feature_cols, interaction, nonlinear, group)\n","    X, y, group_ids, extra = X.to(device), y.to(device), group_ids.to(device), extra.to(device)\n","    use_gamma1, use_gamma2 = interaction, nonlinear\n","\n","\n","    theta = init_theta(D, G, use_gamma1, use_gamma2).to(device)\n","    theta_path = []\n","    history = {\"logloss\": [], \"brier\": [], \"accuracy\": []}\n","    last_theta_draws = None\n","\n","\n","    bag = deque(maxlen=K_ens)\n","\n","    stopped_at = None\n","    eps = 1e-8\n","\n","    for t in range(n_outer):\n","\n","        eta = eta_fn(theta, X, group_ids, extra, D, G, use_gamma1, use_gamma2)\n","        u = sample_truncnorm_from_probit(eta, y).to(device)\n","\n","\n","        cache = imq_cache_from_u(u, c=None, beta=imq_beta)\n","\n","\n","        theta_draws = nuts_sample_theta(\n","            theta_init=theta,\n","            X=X, y=y, group_ids=group_ids, extra=extra,\n","            D=D, G=G, use_gamma1=use_gamma1, use_gamma2=use_gamma2,\n","            u_imputed=u, cache=cache,\n","            beta_lr=beta_lr, prior_std=prior_std,\n","            num_warmup=nuts_warmup, num_samples=nuts_samples,\n","            target_accept_prob=target_accept_prob, max_tree_depth=max_tree_depth,\n","        )\n","        last_theta_draws = theta_draws.detach().cpu()\n","\n","        theta_inner_mean = theta_draws.mean(0).detach()\n","        if t == 0:\n","            theta = theta_inner_mean\n","        else:\n","            theta = (1.0 - alpha) * theta + alpha * theta_inner_mean\n","\n","        theta_path.append(theta.detach().cpu())\n","\n","        k_pp = max(1, int(nuts_samples * frac_pp_mean))\n","        ps = []\n","        for th in theta_draws[-k_pp:]:\n","            p_th, _ = predict_probit(th, df_train, feature_cols, interaction, nonlinear, group)\n","            ps.append(p_th.to(device))\n","        p_mean_this_outer = torch.stack(ps).mean(0)\n","\n","        bag.append(p_mean_this_outer.detach())\n","        p_ens = torch.stack(list(bag)).mean(0)\n","        y_tr = torch.tensor(df_train[\"y\"].values, dtype=torch.float32, device=device)\n","        acc = ((p_ens > 0.5).float() == y_tr).float().mean().item()\n","        brier = torch.mean((p_ens - y_tr) ** 2).item()\n","        logloss = (-y_tr * torch.log(p_ens.clamp(eps, 1-eps))\n","                   - (1 - y_tr) * torch.log((1 - p_ens).clamp(eps, 1-eps))).mean().item()\n","\n","        history[\"logloss\"].append(logloss)\n","        history[\"brier\"].append(brier)\n","        history[\"accuracy\"].append(acc)\n","\n","        if verbose:\n","            print(f\"[outer {t:03d}] TRAIN (EMA+K-ens) ll={logloss:.4f}  br={brier:.4f}  acc={acc:.4f}\")\n","\n","        if len(history[\"logloss\"]) >= 2*K_ma:\n","            def ma_tail(vals, K): return float(np.mean(vals[-K:]))\n","            ll_r = ma_tail(history[\"logloss\"], K_ma)\n","            ll_p = ma_tail(history[\"logloss\"][:-K_ma], K_ma)\n","            br_r = ma_tail(history[\"brier\"],   K_ma)\n","            br_p = ma_tail(history[\"brier\"][:-K_ma],   K_ma)\n","            ac_r = ma_tail(history[\"accuracy\"], K_ma)\n","            ac_p = ma_tail(history[\"accuracy\"][:-K_ma], K_ma)\n","\n","            ll_impr = (ll_p - ll_r) / max(ll_p, 1e-12)\n","            br_impr = (br_p - br_r) / max(br_p, 1e-12)\n","            ac_impr = (ac_r - ac_p)\n","\n","            stop_ll  = (0 <= ll_impr < tol_logloss)\n","            stop_br  = (0 <= br_impr < tol_brier)\n","            stop_acc = (0 <= ac_impr < tol_acc)\n","\n","            if stop_ll and stop_br and stop_acc:\n","                stopped_at = t\n","                if verbose:\n","                    print(f\"[Early stop @ outer {t}] \"\n","                          f\"Δll={ll_impr:.3%}, Δbr={br_impr:.3%}, Δacc={ac_impr:.3f}\")\n","                break\n","    ps = []\n","    with torch.no_grad():\n","      for th in last_theta_draws:\n","        p_th, _ = predict_probit(th.to(device), df_test, feature_cols, interaction, nonlinear, group)\n","        ps.append(p_th.to(device))\n","\n","    p_test = torch.stack(ps).mean(0)\n","\n","    y_te = torch.tensor(df_test[\"y\"].values, dtype=torch.float32, device=device)\n","    acc_te = ((p_test > 0.5).float() == y_te).float().mean().item()\n","    brier_te = torch.mean((p_test - y_te) ** 2).item()\n","    logloss_te = (-y_te * torch.log(p_test.clamp(eps, 1 - eps))\n","              - (1 - y_te) * torch.log((1 - p_test).clamp(eps, 1 - eps))).mean().item()\n","\n","\n","    return {\n","        \"theta_path\": torch.stack(theta_path),\n","        \"history_train\": history,\n","        \"stopped_at\": stopped_at,\n","        \"final_theta\": theta.detach().cpu(),\n","       \"metrics_test\": {\n","        \"accuracy\": acc_te,\n","        \"brier\": brier_te,\n","        \"logloss\": logloss_te\n","    }\n","    }\n"],"metadata":{"id":"ZitoJI_CZzyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","noise_type = \"normal\"\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=False, nonlinear=False, group=False,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"Jr3Vui1wZ7Zl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","noise_type=\"normal\"\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=False, nonlinear=False, group=True,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"B_u-HDkPkW_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=\"normal\",\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = \"normal\",\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=True, nonlinear=False, group=False,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"8TaYkN2ikXJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","noise_type = \"normal\"\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=False, nonlinear=True, group=False,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"p6A4ALz-kXgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","noise_type = \"normal\"\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=True, nonlinear=True, group=True,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"TNcA9P5FkX0t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yDwH_NQvG6H3"},"source":["# Scenario 2 : Misspecified error distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB73WgIY8Dcd"},"outputs":[],"source":["def generate_noise(noise_type, size):\n","    if noise_type == \"normal\":\n","        return np.random.normal(0, 1, size=size)\n","\n","    elif noise_type == \"t\":\n","        return np.random.standard_t(df=2.2, size=size) * 3.0\n","\n","    elif noise_type == \"cauchy\":\n","        return np.random.standard_cauchy(size=size) * 4.0\n","\n","    elif noise_type == \"contaminated\":\n","        base = np.random.normal(0, 1, size=size)\n","        outlier_idx = np.random.choice(size, size=int(0.2 * size), replace=False)\n","        base[outlier_idx] = np.random.normal(0, 10, size=len(outlier_idx))\n","        return base\n","\n","    else:\n","        raise ValueError(\"Unsupported noise type\")\n","\n","def simulate_dataset_noise(noise_type,n_per_group):\n","    rows = []\n","    noise_vector = generate_noise(noise_type, size=n_groups * n_per_group)\n","    noise_counter = 0\n","\n","    for j in range(n_groups):\n","        group_name = group_labels[j]\n","        group_boost = group_effects[j]\n","\n","        for _ in range(n_per_group):\n","            logins_last_week = np.random.poisson(lam=5)\n","            previous_purchases = np.random.poisson(lam=2)\n","            viewed_target_category = np.random.binomial(1, p=0.5)\n","            discount_received = np.random.binomial(1, p=0.5)\n","\n","            U = (\n","                -4.5\n","                + 0.1 * previous_purchases\n","                + 0.6 * viewed_target_category\n","                + 0.9 * discount_received\n","                + 3 * viewed_target_category * discount_received\n","                + 1.1 * group_boost * discount_received\n","                + 0.13 * previous_purchases**2\n","                + noise_vector[noise_counter]\n","            )\n","            noise_counter += 1\n","\n","            y = 1 if U >0 else 0\n","\n","            rows.append({\n","                \"group_id\": j,\n","                \"group_label\": group_name,\n","                \"logins_last_week\": logins_last_week,\n","                \"previous_purchases\": previous_purchases,\n","                \"viewed_target_category\": viewed_target_category,\n","                \"discount_received\": discount_received,\n","                \"y\": y\n","            })\n","    df_simulated = pd.DataFrame(rows)\n","    scaler = StandardScaler()\n","    df_simulated[[\"logins_last_week\", \"previous_purchases\"]] = scaler.fit_transform(\n","    df_simulated[[\"logins_last_week\", \"previous_purchases\"]])\n","    return df_simulated\n","\n","\n","df_simulated_cauchy_test = simulate_dataset_noise(\"cauchy\",10000)\n","df_simulated_t_test = simulate_dataset_noise(\"t\",10000)\n","df_simulated_contaminated_test = simulate_dataset_noise(\"contaminated\",10000)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVFyRrh-gJAu"},"outputs":[],"source":["df_simulated_contaminated_test[\"y\"].value_counts()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jGlxeukdDXO"},"outputs":[],"source":["def run_experiment_loop2(\n","    seeds,\n","    feature_cols,\n","    noise_type_for_train=\"normal\",\n","    npergroup=n_per_group,\n","    group=False, interaction=False, nonlinear=False,\n","    use_quasi=False, loss_kind=\"bce\",\n","    draws=1000, tune=1000, target_accept=0.9\n","):\n","    results = []\n","\n","\n","    for seed in seeds:\n","        np.random.seed(seed)\n","\n","        df_train = simulate_dataset_noise(noise_type_for_train,npergroup)\n","        group_idx_train = df_train[\"group_id\"].values if group else None\n","        df_test = simulate_dataset_noise(noise_type_for_train, 10000)\n","        X_test = df_test[feature_cols].copy()\n","        y_test = df_test[\"y\"].values\n","        group_idx_test = df_test[\"group_id\"].values if group else None\n","        if interaction:\n","          X_test[\"interaction\"] = X_test[\"viewed_target_category\"] * X_test[\"discount_received\"]\n","        if nonlinear:\n","          X_test[\"purchases_squared\"] = X_test[\"previous_purchases\"] ** 2\n","        X_test = X_test.values.astype(\"float64\")\n","\n","        model, eta, y = define_model(\n","            df_train, feature_cols,\n","            group_idx=group_idx_train,\n","            group=group, interaction=interaction, nonlinear=nonlinear\n","        )\n","\n","        if use_quasi:\n","            trace = run_quasi_model(\n","                model, eta, y,\n","                loss_kind=loss_kind,\n","                draws=draws, tune=tune, target_accept=target_accept,\n","                return_inferencedata=True,     idata_kwargs={\"log_likelihood\": True}\n","            )\n","        else:\n","            trace = run_bayesian_model(\n","                model, eta, y,\n","                draws=draws, tune=tune, target_accept=target_accept,\n","                return_inferencedata=True,     idata_kwargs={\"log_likelihood\": True}\n","            )\n","\n","\n","        beta_da = trace.posterior[\"beta\"].stack(sample=(\"chain\", \"draw\"))\n","\n","        if beta_da.ndim == 2:\n","          beta = beta_da.transpose(\"sample\", ...).values\n","\n","          eta = X_test @ beta.T\n","\n","        else:\n","          beta = beta_da.transpose(\"sample\", ...).values\n","          S, G, K = beta.shape\n","\n","          beta_g = beta[:, group_idx_test, :]\n","\n","          eta = np.einsum(\"snk,nk->ns\", beta_g, X_test)\n","\n","        p_samples = norm.cdf(eta)\n","\n","        p_mean = p_samples.mean(axis=1)\n","\n","        y_pred = (p_mean >= 0.5).astype(int)\n","\n","        acc = accuracy_score(y_test, y_pred)\n","        logloss_val = log_loss(y_test, p_mean, labels=[0,1])\n","        brier = brier_score_loss(y_test, p_mean)\n","\n","        results.append({\n","            \"seed\": seed,\n","            \"acc\": acc,\n","            \"logloss\": logloss_val,\n","            \"brier\": brier,\n","            \"model_type\": \"quasi\" if use_quasi else \"bayes\",\n","            \"loss_kind\": loss_kind if use_quasi else None\n","        })\n","\n","    return pd.DataFrame(results)"]},{"cell_type":"markdown","metadata":{"id":"k99DKpeQRARq"},"source":["## 1. Classical Bayesian model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMqIDup3apY7"},"outputs":[],"source":["df_results_bayes_cauchy = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=False,\n","    noise_type_for_train=\"cauchy\"\n",")\n","\n","print(df_results_bayes_cauchy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOVhBmoBapQS"},"outputs":[],"source":["df_results_bayes_t = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=False,\n","    noise_type_for_train=\"t\"\n",")\n","\n","print(df_results_bayes_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqbLkLe5apKM"},"outputs":[],"source":["df_results_bayes_conta = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=False,\n","    noise_type_for_train=\"contaminated\"\n",")\n","print(df_results_bayes_conta)"]},{"cell_type":"markdown","metadata":{"id":"CqG658m0RDaW"},"source":["## 2. Quasi Bayes model with bce loss function"]},{"cell_type":"code","source":["df_results_quasi_cauchy = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=True,\n","    noise_type_for_train=\"cauchy\"\n",")\n","\n","print(df_results_quasi_cauchy)\n"],"metadata":{"id":"3c3kohLkQqb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_results_quasi_t = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=True,\n","    noise_type_for_train=\"t\"\n",")\n","print(df_results_quasi_t)\n"],"metadata":{"id":"GOf3MyGlQqWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_results_quasi_conta = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=True,\n","    noise_type_for_train=\"contaminated\"\n",")\n","print(df_results_quasi_conta)"],"metadata":{"id":"IiXkL5i_QqPC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qqf7fKxWRKKZ"},"source":["## 3. SPH Bayes model"]},{"cell_type":"code","source":["df_results_sph_cauchy = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=True,\n","    noise_type_for_train=\"cauchy\",loss_kind = \"sph\"\n",")\n","\n","print(df_results_sph_cauchy)"],"metadata":{"id":"hpwD8IpZwBOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_results_sph__t = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=True,\n","    noise_type_for_train=\"t\",loss_kind = \"sph\"\n",")\n","\n","print(df_results_sph__t)"],"metadata":{"id":"EJFpzH5vwBLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQ8ZzDObwe6R"},"outputs":[],"source":["df_results_sph_conta = run_experiment_loop2(\n","    seeds=range(20),\n","    feature_cols=feature_cols,\n","    npergroup = 200,\n","        interaction=True,\n","        nonlinear=True,\n","        group=True,\n","    use_quasi=True,\n","    noise_type_for_train=\"contaminated\",loss_kind = \"sph\"\n",")\n","\n","\n","\n","\n","print(df_results_sph_conta)"]},{"cell_type":"markdown","metadata":{"id":"is9V5vjCROKW"},"source":["## 4. KSD Bayes model"]},{"cell_type":"code","source":["all_metrics = []\n","noise_type = \"cauchy\"\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=True, nonlinear=True, group=True,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"_fQ0eoyCllrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","noise_type = \"t\"\n","for seed in range(3):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=True, nonlinear=True, group=True,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"6bIM-FlmlllH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_metrics = []\n","noise_type = \"contaminated\"\n","for seed in range(10):\n","\n","    np.random.seed(seed); torch.manual_seed(seed)\n","    df_train = simulate_dataset(\n","        noise_type=noise_type,\n","        n_per_group=200\n","    )\n","    df_test = simulate_dataset(\n","        noise_type = noise_type,\n","        n_per_group=10000\n","    )\n","    res = fit_ksd_bayes_nuts_ema_ensemble(\n","        df_train, df_test, feature_cols,\n","        interaction=True, nonlinear=True, group=True,\n","        n_outer=40, nuts_warmup=300, nuts_samples=30,\n","        beta_lr=0.01, target_accept_prob=0.90,\n","        device=\"cuda\", verbose=True\n","    )\n","    all_metrics.append(res[\"metrics_test\"])\n","    print(all_metrics)\n","\n","# 집계\n","df = pd.DataFrame(all_metrics)\n","summary = df.agg(['mean','std','median'])\n","print(summary)\n","print(df)"],"metadata":{"id":"hPV3yu0jlleI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JEuqcyyW4o8b"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","toc_visible":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOEE24uxMhucUDwWz7VUSQ5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}